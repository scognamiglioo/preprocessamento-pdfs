{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a_LROfo-cen"
      },
      "source": [
        "Julia Veloso Dias\n",
        "\n",
        "Projeto Integrador 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoHT9NCeQNyb"
      },
      "source": [
        "# Instalar dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g893BDhblg4H",
        "outputId": "aea51ad7-3210-4c4b-9f9d-2133e692c36c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.40)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, dataclasses-json, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 faiss-cpu-1.12.0 langchain-community-0.3.31 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install langchain langchain-community faiss-cpu sentence-transformers transformers torch accelerate pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVFNF65RQUwG"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zszA9acoQbeN"
      },
      "outputs": [],
      "source": [
        "# bibliotecas padrão\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# nlp\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# transformers e modelos\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZBtTmciQmwD"
      },
      "source": [
        "# Estrutura de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYWlKyV0q4GK"
      },
      "source": [
        "Esta etapa do DocumentAnalyzer tem a funcionalidade de realizar uma análise estrutural e conversão dos documentos institucionais para o formato adequado ao sistema RAG. A classe atua como um \"preparador de documentos\" que examina a qualidade, organização e consistência dos dados de entrada, identificando campos obrigatórios, contando artigos e parágrafos, detectando inconsistências e calculando um score de qualidade.\n",
        "\n",
        "Em seguida, converte essa estrutura em documentos LangChain padronizados, aplicando estratégias diferentes para páginas, artigos e tabelas, enquanto enriquece os metadados para melhor recuperação de informação.\n",
        "\n",
        "Essencialmente, transforma dados brutos em estruturado e otimizado para o pipeline de perguntas e respostas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIKJ4WFPQqTW",
        "outputId": "74405243-8d31-4038-a73d-67fc194005e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encontrados 2 arquivos JSON\n",
            "37 documentos convertidos\n",
            "322 documentos convertidos\n",
            "\n",
            "Total de documentos LangChain: 359\n"
          ]
        }
      ],
      "source": [
        "class DocumentAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.analysis_results = {}\n",
        "\n",
        "    def analyze_document_structure(self, data: Dict, doc_name: str) -> Dict:\n",
        "        \"\"\"Analisa a estrutura do documento\"\"\"\n",
        "        analysis = {\n",
        "            'document_name': doc_name,\n",
        "            'has_required_fields': False,\n",
        "            'missing_fields': [],\n",
        "            'structure_type': 'unknown',\n",
        "            'articles_count': 0,\n",
        "            'paragraphs_count': 0,\n",
        "            'tables_count': 0,\n",
        "            'pages_count': 0,\n",
        "            'inconsistencies': [],\n",
        "            'recommendations': []\n",
        "        }\n",
        "\n",
        "        # campos esperados\n",
        "        required_fields = ['doc_id', 'nome_doc', 'estrutura']\n",
        "        optional_fields = ['versao', 'data_publicacao', 'pagina_inicial', 'pagina_final', 'tables', 'paginas']\n",
        "\n",
        "        # campos obrigatórios\n",
        "        missing_required = [field for field in required_fields if field not in data]\n",
        "        analysis['missing_fields'] = missing_required\n",
        "        analysis['has_required_fields'] = len(missing_required) == 0\n",
        "\n",
        "        # identificar estrutura\n",
        "        if data.get('paginas') and len(data['paginas']) > 0:\n",
        "            analysis['structure_type'] = 'paginas'\n",
        "            analysis['pages_count'] = len(data['paginas'])\n",
        "        elif data.get('estrutura') and len(data['estrutura']) > 0:\n",
        "            analysis['structure_type'] = 'estrutura'\n",
        "        elif data.get('tables') and len(data['tables']) > 0:\n",
        "            analysis['structure_type'] = 'tables'\n",
        "        else:\n",
        "            analysis['structure_type'] = 'minimal'\n",
        "\n",
        "        # conta artigos e paragrafos\n",
        "        if analysis['structure_type'] == 'estrutura':\n",
        "            for article in data['estrutura']:\n",
        "                analysis['articles_count'] += 1\n",
        "                paragraphs = article.get('paragrafos', [])\n",
        "                analysis['paragraphs_count'] += len(paragraphs)\n",
        "\n",
        "                if not article.get('artigo'):\n",
        "                    analysis['inconsistencies'].append(f\"Artigo sem identificador na posição {analysis['articles_count']}\")\n",
        "\n",
        "                # verificar parágrafos vazios\n",
        "                for i, paragraph in enumerate(paragraphs):\n",
        "                    if not paragraph.get('texto') or len(paragraph.get('texto', '').strip()) < 10:\n",
        "                        analysis['inconsistencies'].append(f\"Parágrafo vazio: Artigo {article.get('artigo', '?')} - Parágrafo {i+1}\")\n",
        "\n",
        "        # contar tabelas\n",
        "        if data.get('tables'):\n",
        "            analysis['tables_count'] = len(data['tables'])\n",
        "            for i, table in enumerate(data['tables']):\n",
        "                if not table or len(table) == 0:\n",
        "                    analysis['inconsistencies'].append(f\"Tabela {i+1} vazia\")\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def convert_to_langchain_documents(self, data: Dict, doc_name: str) -> List[Document]:\n",
        "        \"\"\"Converte para estrutura LangChain com metadados ricos\"\"\"\n",
        "        documents = []\n",
        "\n",
        "        # conteúdo por páginas\n",
        "        if data.get('paginas'):\n",
        "            for page in data['paginas']:\n",
        "                content = self._extract_page_content(page)\n",
        "                if content:\n",
        "                    metadata = {\n",
        "                        'doc_id': data.get('doc_id', 'unknown'),\n",
        "                        'doc_name': doc_name,\n",
        "                        'page_number': page.get('page', 'unknown'),\n",
        "                        'chapter': page.get('chapter', ''),\n",
        "                        'article': page.get('article', ''),\n",
        "                        'content_type': 'page',\n",
        "                        'source': f\"{doc_name} - Página {page.get('page', '?')}\"\n",
        "                    }\n",
        "                    documents.append(Document(page_content=content, metadata=metadata))\n",
        "\n",
        "        # conteúdo estruturado\n",
        "        elif data.get('estrutura'):\n",
        "            for article in data['estrutura']:\n",
        "                content = self._extract_article_content(article)\n",
        "                if content:\n",
        "                    metadata = {\n",
        "                        'doc_id': data.get('doc_id', 'unknown'),\n",
        "                        'doc_name': doc_name,\n",
        "                        'article': article.get('artigo', ''),\n",
        "                        'chapter': article.get('capitulo', ''),\n",
        "                        'section': article.get('secao', ''),\n",
        "                        'content_type': 'article',\n",
        "                        'source': f\"{doc_name} - {article.get('artigo', 'Artigo')}\"\n",
        "                    }\n",
        "                    documents.append(Document(page_content=content, metadata=metadata))\n",
        "\n",
        "        # tabelas\n",
        "        if data.get('tables'):\n",
        "            for i, table in enumerate(data['tables']):\n",
        "                table_content = self._extract_table_content(table)\n",
        "                if table_content:\n",
        "                    metadata = {\n",
        "                        'doc_id': data.get('doc_id', 'unknown'),\n",
        "                        'doc_name': doc_name,\n",
        "                        'table_index': i,\n",
        "                        'content_type': 'table',\n",
        "                        'source': f\"{doc_name} - Tabela {i+1}\"\n",
        "                    }\n",
        "                    documents.append(Document(page_content=table_content, metadata=metadata))\n",
        "\n",
        "        return documents\n",
        "\n",
        "    def _extract_page_content(self, page: Dict) -> str:\n",
        "        \"\"\"Extrai conteúdo de uma página\"\"\"\n",
        "        content_parts = []\n",
        "\n",
        "        # prioridade: texto limpo → texto bruto\n",
        "        if page.get('clean_text'):\n",
        "            content_parts.append(page['clean_text'])\n",
        "        elif page.get('raw_text'):\n",
        "            content_parts.append(page['raw_text'])\n",
        "\n",
        "        # metadados estruturais\n",
        "        if page.get('chapter'):\n",
        "            content_parts.append(f\"[Capítulo: {page['chapter']}]\")\n",
        "        if page.get('article'):\n",
        "            content_parts.append(f\"[Artigo: {page['article']}]\")\n",
        "\n",
        "        return \" \".join(content_parts) if content_parts else \"\"\n",
        "\n",
        "    def _extract_article_content(self, article: Dict) -> str:\n",
        "        \"\"\"Extrai conteúdo de um artigo\"\"\"\n",
        "        content_parts = []\n",
        "\n",
        "        # cabeçalho do artigo\n",
        "        if article.get('artigo'):\n",
        "            content_parts.append(article['artigo'])\n",
        "\n",
        "        if article.get('capitulo'):\n",
        "            content_parts.append(f\"Capítulo: {article['capitulo']}\")\n",
        "\n",
        "        if article.get('secao'):\n",
        "            content_parts.append(f\"Seção: {article['secao']}\")\n",
        "\n",
        "        # parágrafos\n",
        "        for paragraph in article.get('paragrafos', []):\n",
        "            if paragraph.get('texto') and len(paragraph['texto'].strip()) > 5:\n",
        "                para_text = paragraph['texto']\n",
        "                if paragraph.get('numero'):\n",
        "                    para_text = f\"{paragraph['numero']} {para_text}\"\n",
        "                content_parts.append(para_text)\n",
        "\n",
        "        return \"\".join(content_parts) if content_parts else \"\"\n",
        "\n",
        "    def _extract_table_content(self, table: List) -> str:\n",
        "        \"\"\"Extrai conteúdo de tabela de forma inteligente\"\"\"\n",
        "        if not table or not isinstance(table, list):\n",
        "            return \"\"\n",
        "\n",
        "        content_lines = []\n",
        "\n",
        "        for row in table:\n",
        "            if isinstance(row, list):\n",
        "                # filtrar células vazias e juntar\n",
        "                cells = [str(cell).strip() for cell in row if cell and str(cell).strip()]\n",
        "                if cells and not self._is_institutional_row(cells):\n",
        "                    content_lines.append(\" | \".join(cells))\n",
        "            elif isinstance(row, str) and row.strip():\n",
        "                if not self._is_institutional_row([row]):\n",
        "                    content_lines.append(row.strip())\n",
        "\n",
        "        return \" \".join(content_lines) if content_lines else \"\"\n",
        "\n",
        "    def _is_institutional_row(self, cells: List[str]) -> bool:\n",
        "        \"\"\"Identifica linhas institucionais irrelevantes\"\"\"\n",
        "        text = \" \".join(cells).upper()\n",
        "\n",
        "        institutional_indicators = [\n",
        "            'INSTITUTO FEDERAL', 'CAMPUS', 'DIRETORIA', 'MINISTÉRIO',\n",
        "            'SECRETARIA', 'COORDENAÇÃO', 'REITORIA', 'FORMULÁRIO'\n",
        "        ]\n",
        "\n",
        "        return any(indicator in text for indicator in institutional_indicators)\n",
        "\n",
        "    def generate_analysis_report(self, analyses: Dict) -> pd.DataFrame:\n",
        "        \"\"\"Gera relatório consolidado da análise\"\"\"\n",
        "        report_data = []\n",
        "\n",
        "        for doc_name, analysis in analyses.items():\n",
        "            report_data.append({\n",
        "                'Documento': doc_name,\n",
        "                'Tipo Estrutura': analysis['structure_type'],\n",
        "                'Campos Obrigatórios': 'OK' if analysis['has_required_fields'] else 'Erro',\n",
        "                'Artigos': analysis['articles_count'],\n",
        "                'Parágrafos': analysis['paragraphs_count'],\n",
        "                'Tabelas': analysis['tables_count'],\n",
        "                'Inconsistências': len(analysis['inconsistencies'])\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(report_data)\n",
        "\n",
        "# carregar e analisar documentos\n",
        "def load_and_analyze_documents(json_directory: str) -> Tuple[Dict, List[Document]]:\n",
        "    \"\"\"Carrega e analisa todos os documentos JSON\"\"\"\n",
        "    analyzer = DocumentAnalyzer()\n",
        "    all_documents = []\n",
        "    analyses = {}\n",
        "\n",
        "    json_files = list(Path(json_directory).glob('*.jsonl'))\n",
        "\n",
        "    print(f\"Encontrados {len(json_files)} arquivos JSON\")\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            doc_name = json_file.stem\n",
        "\n",
        "            # Análise da estrutura\n",
        "            analysis = analyzer.analyze_document_structure(data, doc_name)\n",
        "            analyses[doc_name] = analysis\n",
        "\n",
        "            # Conversão para documentos LangChain\n",
        "            documents = analyzer.convert_to_langchain_documents(data, doc_name)\n",
        "            all_documents.extend(documents)\n",
        "\n",
        "            print(f\"{len(documents)} documentos convertidos\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar {json_file}: {e}\")\n",
        "\n",
        "    return analyses, all_documents\n",
        "\n",
        "json_directory = '/content/sample_data/'\n",
        "analyses, documents = load_and_analyze_documents(json_directory)\n",
        "\n",
        "# relatório\n",
        "if analyses:\n",
        "    report_df = DocumentAnalyzer().generate_analysis_report(analyses)\n",
        "    print(f\"\\nTotal de documentos LangChain: {len(documents)}\")\n",
        "else:\n",
        "    print(\"Nenhum documento foi analisado com sucesso\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7-Khby3RQdx"
      },
      "source": [
        "# Embedding e vetorização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAGeSYWxr38-"
      },
      "source": [
        "A classe VectorStoreManager funciona como o parte semântica do sistema RAG, responsável por transformar os documentos textuais em representações numéricas (embeddings) e criar um mecanismo de busca que compreende o significado por trás das palavras.\n",
        "\n",
        "Utilizando o modelo `all-MiniLM-L6-v2` para gerar embeddings de qualidade e a biblioteca FAISS para indexação vetorial, ela converte todo o conteúdo em um espaço semântico onde documentos com significados similares ficam próximos, permitindo que o sistema recupere informações relevantes mesmo quando as palavras exatas da consulta não estão presentes nos textos originais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649,
          "referenced_widgets": [
            "dfa645efc50643b5a481f952bec12cf3",
            "e4ae6189ee5144649bf497fbdadffaa8",
            "2ef0828dff414fbe88afddc311c7bcf6",
            "326fb009c5ea4d7ca224d3cc4c8930f1",
            "e5c8ec4b76604bbc8e1468e97c723a20",
            "5d3e3f5938d2412ba5c421e18546d880",
            "508dbc5d050146ad921dedffbe9a2124",
            "68df5123d4f9424fbd1cd4f9124e23ea",
            "3a3fced3c10449d48b24a9887521f806",
            "80659727c5744eab9d9c953e8e4be745",
            "eacb92c848fe4edbb35cc39e25be5d6c",
            "20f2c4b5461f41ef888f8f3baf399294",
            "3e2460b67feb46b1bdb42a5315596d24",
            "736188e108964f2497a33694771d3d55",
            "c7f67db875c34a5bbaaa125d297775ca",
            "0eff7de158bb4c81927e039ba2318e66",
            "37666de1560d47b09b202517f2756b55",
            "c7c3b76c6d264eb69542f0078bf6cf75",
            "a2d1633a6b354111bda7aa1ef79aca55",
            "a768938b28184294a1bf6a2ab475e804",
            "4dcfae7be28f43ef9278ec73cfa871c2",
            "89a37f31ff1740a59ce5ea3d183c412a",
            "e86b9e46c7344c519b6ef213a41bd290",
            "b3933ca10f08403dae64593cadaa737f",
            "5ce4a68c73c84fba84b47ded047acc9a",
            "678404004ac44400864ffa310d3ddc29",
            "610cbb646e234ee6bc9dee9521da4ac5",
            "80f5a3296f0b47b8aad69fb90cb9123f",
            "15daf445994a4387b94c08acd5aa7ea1",
            "baec02682b804d5ba0898c9d76375468",
            "cd9f7c1e6622471b998fa71c17979624",
            "3cd491f39bf2407d9ee5129dc7bf552f",
            "4d1ab60fb4d94c4aadfcff367a8033e3",
            "c84a03e43f844d1d8e935543ebebb202",
            "9152dddf69844194aa9525cbdd25ec29",
            "ef04a666e797493eafe993adf9181ebf",
            "90bd8ded21394cf4ac0082b784c0071e",
            "b3fcd95853024bba9e56cdc184dfca56",
            "0096cc3362784001b91c7c8704deb660",
            "339a1f3c091b43b1b8ee516a329742c7",
            "c6499b332b084789bea90daecd9014d6",
            "104fea554dad48b8a2c42d524dd4232a",
            "98b793e57d544516bfac41084c5ba743",
            "d257e35f10dd4505bb5b2d848df51abc",
            "2d731d06ef634fbabf8c5fa7efa1db05",
            "61baa1e9b3c6459f86b520bf4bbc8d97",
            "b92407f2b1cc46b8b03a3a662371d010",
            "a6e7028f7d8a496ab3e2faecc7359c06",
            "4a52f36b9ce4417b98755d4aad3edf3b",
            "8948e1ab856a457aa4f233a6bff054a9",
            "63cb03d4b50d4530913c95836b9955a5",
            "85326f133055474594ab54dd1c896ebf",
            "06ed3c8a2fce484c96ebea51cba8286c",
            "5748e8af384842fe9736fb6a119d872a",
            "4a712e311f6c4539814267631f26c670",
            "e42de52a3fef47faa020469d48be7317",
            "dca22003e67c4606ac3d61f8c5ab5388",
            "8ca2a8b71fc84ccf883a4d912d1f6498",
            "0b432ca05d11421da58a27a84dbf86ab",
            "c02389f2439b44e29a28f3df294e931c",
            "9ea83be71c9e4704a6b8d65413712848",
            "657995c6b0b74b018508c9fe36b1b638",
            "5fdd95cf17584e7fa8cf8511c372cce8",
            "e479af8222634f5d9e9c76dcb9c0bd51",
            "a29220b7ed4b474eb3959364e2524501",
            "9a4a46e815eb4118829a6a0757644463",
            "240c531c186849dbaf003561423ca8ea",
            "ed510c9fc0a94b408db09de63d242854",
            "c16a0e6b31e74da0860d11d204ef1d52",
            "17836ef872c34f2d9973551ab4d30e9c",
            "670d74f95fbb4c2bb71449af39c64228",
            "68577ad0efd64c50a64b5743dbdc7776",
            "1eb46f79b3bd49ddb6ae778652979fd2",
            "0c0e356955cd42dd9d3fcb0c8435e874",
            "01d0e994a7044668abc8c63bf9ac459c",
            "fe7a3e2f9e0f4c07944e341ada863f4c",
            "760b5f7ec2eb4b338a350eff0122f82d",
            "dca376e31f5549a48f3b4f92410d3cba",
            "810be18add884488bf39daec1d712784",
            "8d88aaec7c3f466c85a28a9899b89ed5",
            "fd33ce7ef8594e7da7c032dbee1edf98",
            "1e5466849d4e44eea8ed30fd3719ae0f",
            "f1bb106a46f644b7ae94c7585f1b6fd9",
            "1a8383177e6c4b0b8d3b41ab7ac4e020",
            "52c80637be7e42bba5771bc95639029b",
            "914f2ed6d2084cdf89513f299724e4df",
            "f1c35b180667415f9a9db237d49f3d29",
            "f3f0a97a9c6d437a8eca5fb32d60d1e3",
            "eb1996224abf4658b0f19333884de73d",
            "2a49cc219f054ce6b1c549bbd330c951",
            "a61eb10a11af4a4aac07ce22108295e7",
            "739241386a0b4744b9a4d257b6b318a7",
            "acefa85c678b401ebbefb0ae289d6a51",
            "7005e27008f0446987619e19b4865f7c",
            "665354ac688442c48e0f7090e305ed9c",
            "9698b8b8782f4e1aa0ef2673a8a72165",
            "58245f3764b048a1bd54e76f2e5ae61f",
            "99e557cd8d8242efac37912a6ee6d2ca",
            "044f3f548b344cadbd73f235fe5196d8",
            "6d0b15b8fed044cbbfdbe527ffb98e24",
            "80960bc7690849bfb57033c406518314",
            "82bc3f9faf9c4824a1c5b9495cc2de6d",
            "bbaf6d2347c34676bbe7ea29ffc332a4",
            "40886c4de044425ab54ef3f99110267d",
            "9c3d4e19bdcb42d880e5e34c442accff",
            "ca32a8f0fd6d4bb2bb08b3a84934e88b",
            "89fd321f23144c58bb92a2bce1caf04d",
            "cbbfa758588c49f89e4d987532aaf162",
            "2a6bb41a924149f9b65538b79755cb11",
            "b191b7623cf747c39e241388eee47611",
            "e0648a41569b4421ba593e8931493fcb",
            "ce6d127a8491435095ed301758bcf7d3",
            "df8dde278173467c89023a3d5f482ef9",
            "23192f74b79e46e4829a49c6033cef6e",
            "2c16a23aec044c44b0de3c59a61ba4f4",
            "f2520534c24f4b74ac0f54a7d4c3f96b",
            "8344a57362c34161b1a80f1d92fdc84e",
            "699dd8fa660f4790bfa31960a43d6bf2",
            "9e66fb788bf54e53a1802cea4b8ec78e",
            "86eeada8be9941548b392bd31964439b",
            "779c3a9e30cf441488050d35006684da"
          ]
        },
        "id": "bJMNM1jURPSp",
        "outputId": "5760f647-1d57-4bce-bf45-6e1e5e656f94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3128124425.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  self.embeddings = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfa645efc50643b5a481f952bec12cf3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20f2c4b5461f41ef888f8f3baf399294",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e86b9e46c7344c519b6ef213a41bd290",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c84a03e43f844d1d8e935543ebebb202",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d731d06ef634fbabf8c5fa7efa1db05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e42de52a3fef47faa020469d48be7317",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "240c531c186849dbaf003561423ca8ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dca376e31f5549a48f3b4f92410d3cba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb1996224abf4658b0f19333884de73d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d0b15b8fed044cbbfdbe527ffb98e24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0648a41569b4421ba593e8931493fcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings configurado\n",
            "Vector store criado: 359\n",
            "Dimensões dos embeddings: 384\n",
            "Total de vetores no índice: 359\n",
            "\n",
            "Teste de busca: 'tempo máximo conclusão curso'\n",
            "Documentos encontrados: 3\n",
            "   1. ppc_output - Tabela 202\n",
            "      2 IDENTIFICAÇÃO DO CURSO Denominação do Curso: Ciência da Computação Carga horária total: 3.280 hora...\n",
            "   2. estagio_output - Artigo\n",
            "      DO INÍCIO E DURAÇÃO...\n",
            "   3. ppc_output - Tabela 300\n",
            "      6.5.13 Trabalho de conclusão de curso O Trabalho de Conclusão de Curso (TCC) é um requisito curricul...\n"
          ]
        }
      ],
      "source": [
        "class VectorManager:\n",
        "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        self.model_name = model_name\n",
        "        self.embeddings = None\n",
        "        self.vectorstore = None\n",
        "\n",
        "    def create_embeddings(self):\n",
        "        \"\"\"Cria os embeddings usando sentence-transformers\"\"\"\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=self.model_name,\n",
        "            model_kwargs={'device': 'cpu'},\n",
        "            encode_kwargs={'normalize_embeddings': True}\n",
        "        )\n",
        "        print(\"Embeddings configurado\")\n",
        "\n",
        "    def create_vector_store(self, documents: List[Document]):\n",
        "        \"\"\"Cria o vector store com FAISS\"\"\"\n",
        "        if not documents:\n",
        "            raise ValueError(\"Nenhum documento fornecido\")\n",
        "\n",
        "        self.vectorstore = FAISS.from_documents(documents, self.embeddings)\n",
        "        print(f\"Vector store criado: {len(documents)}\")\n",
        "\n",
        "        # Estatísticas do vector store\n",
        "        index = self.vectorstore.index\n",
        "        print(f\"Dimensões dos embeddings: {index.d}\")\n",
        "        print(f\"Total de vetores no índice: {index.ntotal}\")\n",
        "\n",
        "    def search_similarity(self, query: str, k: int = 5) -> List[Document]:\n",
        "        \"\"\"Busca por similaridade semântica\"\"\"\n",
        "        if not self.vectorstore:\n",
        "            raise ValueError(\"Vector store não inicializado\")\n",
        "\n",
        "        return self.vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "    def get_retriever(self, search_type: str = \"similarity\", k: int = 4):\n",
        "        \"\"\"Retorna um retriever configurado\"\"\"\n",
        "        search_kwargs = {\"k\": k}\n",
        "\n",
        "        if search_type == \"mmr\":\n",
        "            search_kwargs.update({\"fetch_k\": 10, \"lambda_mult\": 0.7})\n",
        "\n",
        "        return self.vectorstore.as_retriever(\n",
        "            search_type=search_type,\n",
        "            search_kwargs=search_kwargs\n",
        "        )\n",
        "\n",
        "vector_manager = VectorManager()\n",
        "\n",
        "# criar embeddings\n",
        "vector_manager.create_embeddings()\n",
        "\n",
        "# criar vector store\n",
        "vector_manager.create_vector_store(documents)\n",
        "\n",
        "# testar busca\n",
        "test_query = \"tempo máximo conclusão curso\"\n",
        "similar_docs = vector_manager.search_similarity(test_query, k=3)\n",
        "print(f\"\\nTeste de busca: '{test_query}'\")\n",
        "print(f\"Documentos encontrados: {len(similar_docs)}\")\n",
        "\n",
        "for i, doc in enumerate(similar_docs):\n",
        "    print(f\"   {i+1}. {doc.metadata['source']}\")\n",
        "    print(f\"      {doc.page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlGGBMk2RX7t"
      },
      "source": [
        "# Modelos LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t8sfStauFU2"
      },
      "source": [
        "Esta função tem como objetivo configurar e inicializar o modelo de linguagem DialoGPT-medium para ser utilizado no sistema de perguntas e respostas.\n",
        "\n",
        "Ela carrega o tokenizer e o modelo pré-treinado da Microsoft, realiza configurações essenciais como a definição do token de padding (crítico para evitar erros de processamento) e cria um pipeline de geração de texto com parâmetros otimizados para conversação - incluindo controle de criatividade (temperature), diversidade lexical (top_p), prevenção de repetições (repetition_penalty) e limites de comprimento para garantir respostas coerentes e contextualizadas.\n",
        "\n",
        "A função encapsula toda essa complexidade em uma interface simples que retorna um objeto LLM pronto para ser integrado ao pipeline RAG, proporcionando ao sistema a capacidade de gerar respostas conversacionais naturais baseadas nos documentos recuperados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FNUgbV5Lxd4L"
      },
      "outputs": [],
      "source": [
        "def setup_dialogpt_model():\n",
        "    \"\"\"Configura o modelo DialoGPT\"\"\"\n",
        "    print(\"--- DIALOGPT ---\")\n",
        "\n",
        "    try:\n",
        "        from langchain.llms import HuggingFacePipeline\n",
        "        from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "        model_name = \"microsoft/DialoGPT-large\"\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "        # configurar pipeline de geração com parâmetros\n",
        "        text_generator = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=300,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            truncation=True,\n",
        "            max_length=2048\n",
        "        )\n",
        "\n",
        "        llm = HuggingFacePipeline(pipeline=text_generator)\n",
        "        print(\"OK\")\n",
        "        return llm\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsaftckbxiXx"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUc5U1PwuXN2"
      },
      "source": [
        "Esta função implementa o núcleo do sistema RAG (Retrieval-Augmented Generation) conectando o mecanismo de recuperção de informações com o modelo de linguagem para gerar respostas contextualizadas.\n",
        "\n",
        "Ela cria um pipeline onde, para cada pergunta, o sistema primeiro recupera os documentos mais relevantes do banco vetorial (usando similaridade semântica e buscando os 5 melhores resultados) e depois injeta esse contexto como base para o modelo DialoGPT gerar uma resposta precisa e fundamentada.\n",
        "\n",
        "A função emprega uma abordagem de \"stuffing\" onde todo o contexto recuperado é inserido diretamente no prompt do modelo, seguido pela pergunta do usuário, criando uma estrutura limpa que permite ao LLM acessar as informações relevantes dos documentos institucionais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TygEJwdNxkO4"
      },
      "outputs": [],
      "source": [
        "def setup_rag_pipeline(vector_store_manager, llm):\n",
        "    \"\"\"Configura o pipeline RAG completo com tratamento de erro\"\"\"\n",
        "    print(\"Configurando pipeline RAG...\")\n",
        "\n",
        "    from langchain.chains import RetrievalQA\n",
        "    from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "\n",
        "{context}\n",
        "\n",
        "Pergunta: {question}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Configurar retriever com menos documentos para evitar sobrecarga\n",
        "        retriever = vector_store_manager.get_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            k=5\n",
        "        )\n",
        "\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=retriever,\n",
        "            chain_type_kwargs={\"prompt\": PROMPT},\n",
        "            return_source_documents=True\n",
        "        )\n",
        "\n",
        "\n",
        "        return qa_chain\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro {e}\")\n",
        "        return setup_simple_qa_chain(llm)\n",
        "\n",
        "def setup_simple_qa_chain(llm):\n",
        "    \"\"\"Configura uma cadeia QA simples como fallback\"\"\"\n",
        "    from langchain.chains import LLMChain\n",
        "    from langchain.prompts import PromptTemplate\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"question\"],\n",
        "        template=\"Responda à pergunta: {question}\"\n",
        "    )\n",
        "\n",
        "    class SimpleQAChain:\n",
        "        def __init__(self, chain):\n",
        "            self.chain = chain\n",
        "\n",
        "        def __call__(self, inputs):\n",
        "            result = self.chain.run(inputs[\"query\"])\n",
        "            return {\n",
        "                \"result\": result,\n",
        "                \"source_documents\": []\n",
        "            }\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    return SimpleQAChain(chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC6Z4ST9xp2e"
      },
      "source": [
        "# PERGUNTA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYyH4C3GurXL"
      },
      "source": [
        "Esta função implementa a interface principal de consulta ao sistema RAG, funcionando como o ponto de interação entre o usuário e a inteligência artificial.\n",
        "\n",
        "Ela recebe perguntas, as submete ao pipeline RAG configurado e retorna respostas contextualizadas com transparência sobre as fontes consultadas. A função incorpora um sofisticado mecanismo de timeout (190 segundos) que monitora o tempo de processamento para evitar travamentos, garantindo responsividade mesmo com consultas complexas.\n",
        "\n",
        "Após processar cada pergunta, ela exibe de forma organizada tanto a resposta gerada pelo modelo quanto a lista detalhada dos documentos institucionais que fundamentaram a resposta, incluindo previews do conteúdo recuperado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAVCsxACRc5X",
        "outputId": "b807319e-d97c-4bab-9996-37c7e244c490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CONFIGURANDO MODELO...\n",
            "--- DIALOGPT ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n",
            "\n",
            "CONFIGURANDO PIPELINE RAG...\n",
            "Configurando pipeline RAG...\n",
            "\n",
            "EXECUTANDO 5 PERGUNTAS...\n",
            "\n",
            "============================================================\n",
            "PERGUNTA 1: Quais são os professores que podem ser orientadores do TCC?\n",
            "============================================================\n",
            "\n",
            "**RESPOSTA:** \n",
            "\n",
            "de TCC. E\n",
            "m resumo, compete ao aluno após a realização da disciplina de PTCC:   Cumprir as etapas de trabalho estabelecidas no cronograma do projeto.   Cumprir o calendário estabelecido pelo professor orientador.   Comparecer aos horários de orientação definidos pelo professor orientador.   Realizar as atividades previstas no projeto.   Elaborar monografia redigida de acordo com as Normas Institucionais e com o modelo apre- sentado na disciplina de PTCC.   Entregar uma cópia do trabalho a cada membro da banca de avaliação, obedecendo aos pra- zos estipulados pelo calendário acadêmico e pelo professor orientador.   Apresentar oralmente o TCC perante banca de professores para avaliação.   Analisar juntamente com o professor orientador as sugestões da banca e efetuar as correções e adequações julgadas pertinentes.   Entregar ao professor orientador uma cópia digital e uma cópia impressa devidamente assi-\n",
            "\n",
            " | Informar imediatamente ao professor da disciplina a eventual necessidade de troca de pro- fessor orientador ou tema do TCC.   Comparecer aos horários de orientação definidos pelo professor orientador.   Cumprir o calendário estabelecido pelo professor da disciplina.   Elaborar o projeto do TCC, de acordo com o modelo estabelecido.   Entregar uma cópia do projeto a cada membro da banca de avaliação, obedecendo aos pra- zos estipulados pelo calendário da disciplina.   Apresentar oralmente o projeto perante banca de professores para qualificação do TCC.   Analisar, juntamente, com o professor orientador as sugestões da banca e efetuar as corre- ções e adequações julgadas pertinentes.   Entregar  ao professor de  PTCC uma cópia digital  do projeto  na versão  final  para  arquiva-\n",
            "\n",
            "Após a aprovação pela banca de avaliação do projeto de TCC, o aluno deve executar o pro- jeto, onde sob a orientação do mesmo professor orientador, salvo casos excepcionais, o aluno deve- rá seguir o cronograma estipulado no projeto. Considerando os dias letivos do calendário acadêmico do semestre posterior à aprovação pela banca em PTCC, o aluno deve desenvolver o trabalho pro- posto, além de redigir um trabalho monográfico o qual deverá ser apresentado, novamente, em ban- ca composta de pelo menos três professores, sendo obrigatória a participação do orientador e coori- entador, caso haja (a presença do coorientador eleva o número de participantes na banca). A apre- sentação do TCC poderá ocorrer em qualquer momento do semestre, dentro dos dias letivos defini- dos no calendário acadêmico, sendo de responsabilidade do professor orientador agendar os materi- ais necessários para apresentação.  A nota do TCC será de no máximo 100 pontos, distribuídos to- talmente  pela  banca  examinadora  de  TCC,  sendo  a  comprovação  de  conclusão  de  TCC  a  ata  de acadêmicos para lançamento da nota e cômputo das horas necessárias para finalização do requisito de TCC.\n",
            "\n",
            "DO PROFESSOR-ORIENTADOR\n",
            "\n",
            "isciplina: \n",
            "Nº aulas semanais: \n",
            "Carga horária: \n",
            "Projeto de Trabalho de Conclusão de Curso \n",
            "4 \n",
            "72 h/a Ementa: \n",
            "Discussão  de \n",
            "temas \n",
            "relacionados  aos  campos  da  Ciência  da  Computação.  Orientações  para \n",
            "elaboração de projeto de investigação acerca de um tema de livre escolha do acadêmico, considerado \n",
            "os  temas  previamente  discutidos  e  supervisionado  pelo  professor  orientador. Apresentação  pública \n",
            "do projeto a ser elaborado a uma banca composta pelo orientador e por outros professores, conforme \n",
            "regulamento. Bibliografia Básica: \n",
            "WAZLAWICK, Raul Sidnei. Metodologia de Pesquisa em Ciência da Computação. Rio de Janei-\n",
            "\n",
            "Pergunta: Quais são os professores que podem ser orientadores do TCC?\n",
            "\n",
            "\n",
            "\n",
            "FONTES CONSULTADAS (5):\n",
            "   1. [TABLE] ppc_output - Tabela 304\n",
            "      de TCC. E\n",
            "m resumo, compete ao aluno após a realização da disciplina de PTCC:  ...\n",
            "   2. [TABLE] ppc_output - Tabela 302\n",
            "       | Informar imediatamente ao professor da disciplina a eventual necessidade de ...\n",
            "   3. [TABLE] ppc_output - Tabela 303\n",
            "      Após a aprovação pela banca de avaliação do projeto de TCC, o aluno deve executa...\n",
            "   4. [ARTICLE] estagio_output - Artigo\n",
            "      DO PROFESSOR-ORIENTADOR\n",
            "   5. [TABLE] ppc_output - Tabela 77\n",
            "      isciplina: \n",
            "Nº aulas semanais: \n",
            "Carga horária: \n",
            "Projeto de Trabalho de Conclusão...\n",
            "\n",
            "============================================================\n",
            "PERGUNTA 2: Qual é o tempo máximo permitido para que um estudante conclua o curso\n",
            "============================================================\n",
            "\n",
            "**RESPOSTA:** \n",
            "\n",
            "O TCC é composto por um total de 120 horas, oferecidas aos alunos do curso de Ciência da Computação no 9º e 10º períodos, através da disciplina Projeto de Trabalho de Conclusão de Curso com  60  horas  e  da  apresentação  do  Trabalho  de  Conclusão  de  Curso  perante  a  banca  avaliadora, sendo contabilizadas outras 60 horas após a aprovação da banca. O TCC possui três papeis fundamentais:   Aluno:  aluno  do  curso  de  Ciência  da  Computação  regularmente  matriculado  na  disciplina Projeto de Trabalho de Conclusão de Curso (PTCC). Após a aprovação na disciplina PTCC, o aluno deve desenvolver (ou construir), sob orientação do professor orientador o trabalho de  conclusão  de  curso  de  acordo  com  as  normas  estabelecidas  e  especificidade  do  projeto definido.   Professor de TCC: papel desempenhado pelo professor das disciplinas de Projeto de Traba- lho de Conclusão de Curso, o qual é responsável por passar as diretrizes de construção do trabalho  de  conclusão  aos  alunos,  juntamente  com  as  normas  técnicas  e  lançar  as  notas  e frequências no diário, receber os trabalhos finais e enviar para publicação na biblioteca. orientar o aluno na construção do projeto de TCC, acompanhar a execução e agendar a ban- ca para avaliação do trabalho final de TCC. O\n",
            " TCC será orientado por um professor pertencente ao quadro docente do IFNMG – Cam- pus Montes Claros e que possua conhecimentos na área de ciência da computação de interesse do acadêmico.  Além disso, o aluno poderá ser coorientado por outro profissional. A disciplina de Projeto de Trabalho de Conclusão de Curso (PTCC) consiste na elaboração do  projeto  do TCC  que  será  desenvolvido  pelo  aluno,  devendo  o  projeto  ser  apresentado  perante banca de professores para julgar a pertinência do trabalho proposto, juntamente, com os prazos do cronograma estabelecido. A banca será composta pelo professor da disciplina, pelo professor orien- tador, coorientador (se houver), e mais um professor do IFNMG. A nota da disciplina será de 100 pontos, distribuídos da seguinte maneira: 30 pontos pelo professor da disciplina referente ao acom- panhamento das atividades propostas na disciplina e 70 pontos pela banca avaliadora. Compete ao aluno matriculado em PTCC:   Assistir às aulas teóricas e realizar as tarefas determinadas pelo professor.  | Informar ao professor da disciplina o nome do professor orientador escolhido e um tema pa- ra pesquisa, dentro do prazo estipulado pelo professor da disciplina. Página 87 de 103\n",
            "\n",
            "2 IDENTIFICAÇÃO DO CURSO Denominação do Curso: Ciência da Computação Carga horária total: 3.280 horas Modalidade: Presencial Tipo: Bacharelado Ano de implantação: 2013 Titulação conferida: Bacharel em Ciência da Computação Turno de oferta: Diurno Integral Regime Acadêmico: Semestral Número de vagas oferecidas: 40 vagas Periodicidade de oferta: Anual Requisitos de acesso: Ensino Médio Completo Forma de ingresso: Vestibular e SISU Duração do curso: Cinco anos Prazo para integralização: Mínimo de cinco anos (5) e máximo de sete anos e meio (7,5) Autorização para funcionamento: Portaria Nº 521 – Reitor/2012 Montes Claros - MG\n",
            "\n",
            "§  3º  Fica  aprovado  o  estágio  em  regime  remoto  ou  teletrabalho  no  âmbito  do  IFNMG, desde  que  esteja  disciplinado  e  previsto  no  Projeto  Pedagógico  do  Curso.  (incluído  pela  Resolução Consup nº 329, de 4 de maio de 2023) CAPÍTULO IV DAS CONDIÇÕES PARA REALIZAÇÃO DO ESTÁGIO A\n",
            "rt.  4º  Para  a  realização  do  estágio  curricular,  obrigatório  ou  não,  o  estudante  deverá I - Requerimento de Estágio (em duas vias); II - Termo de Compromisso de Estágio (em três vias, assinadas pelo discente, pela unidade concedente e pela ins\u0000tuição de ensino); III  -  Plano  de  Estágio  (em  três  vias,  assinadas  pelo  discente,  pelo  supervisor  da  unidade concedente,  pelo  professor  orientador  da  ins\u0000tuição  de  ensino  e  pelo  coordenador  do  curso,  ou  cargo equivalente). §\n",
            " 1º A realização dos estágios mencionados no  caput  se aplica aos discentes em curso e aos  que | já  concluíram  a  carga  horária  de  disciplinas,  conforme  previsto  no  Projeto  Pedagógico, obedecendo-  se  as  condições  e  prazos  nele  estabelecidos  e/ou  regulamentação  própria  do  estágio  no curso. §  2º  Somente  serão  aceitos  registros  de  estágios  de  alunos  dos  cursos  técnicos  de  nível médio, graduação e pós-graduação (lato ou stricto sensu), que indicarem, em seus Projetos Pedagógicos, a possibilidade da realização de estágio curricular, obrigatório ou não. §  3º  O  discente  que  iniciar  o  estágio  regular  sem  que  tenha  efetuado  seu  requerimento perderá  o  tempo  de  estágio  realizado,  anteriormente  à  data  do  mesmo,  exceto  para  os  estudantes  em mobilidade  internacional,  que  obedecerão  ao  Capítulo  V,  e  aqueles  em  aproveitamento  de  a\u0000vidades, que seguirão o Capítulo IX deste regulamento. A\n",
            "rt.  5º  O  estágio  não  obrigatório  será \n",
            "realizado  enquanto  o  discente  se  man\u0000ver matriculado e frequente na Ins\u0000tuição. Parágrafo  único.  Os  procedimentos  para  requerer  o  estágio  não  obrigatório  seguirão  as mesmas normas estabelecidas para o estágio obrigatório. CAPÍTULO V DA REALIZAÇÃO Art.  6º  O  estágio  será  realizado  em  en\u0000dades  concedentes,  ou  por  meio  de  agências  de integração | públicas \n",
            "ou \n",
            "privadas, \n",
            "devidamente \n",
            "conveniadas \n",
            "e/ou \n",
            "cadastradas \n",
            "no \n",
            "IFNMG, \n",
            "que apresentarem condições de proporcionar experiências prá\u0000cas na área de formação do discente. §  1º  Entende-se  por  en\u0000dade  concedente:  empresas,  ins\u0000tuições  públicas  ou  privadas, terceiro  setor  (associações  sem  ﬁns  lucra\u0000vos)  e  proﬁssionais  liberais  de  nível  superior  (devidamente registrados em seus respec\u0000vos conselhos de ﬁscalização proﬁssional, quando houver). §  2º  O  estágio  poderá  ser \n",
            "realizado  no  próprio \n",
            "IFNMG,  desde  que  as  a\u0000vidades desenvolvidas  assegurem  o  alcance  dos  obje\u0000vos  previstos  no  art.  2º  deste  regulamento,  devidamente\n",
            "\n",
            "De acordo com o Regulamento dos Coordenadores de Curso de Graduação do IFNMG, são atribuições do coordenador de curso:   manter-se permanentemente atualizado quanto à legislação e normas regulamentares vigentes, e zelar pelo seu cumprimento;  | realizar, em conjunto com a equipe de supervisão pedagógica e Direção de Ensino, reunião de recepção dos discentes de novas turmas, para sensibilização e orientação acerca da matriz curricular do curso e das normas e regulamentos institucionais;  | representar e fazer representar o curso que coordena em atos públicos e nas relações com outras instituições acadêmicas, profissionais ou científicas;   zelar pelo  cumprimento  dos planos de curso ou programas de curso, administrando suas alterações;  | supervisionar  e  coordenar  o  funcionamento  do  curso,  acompanhando  as  atividades dos trabalhos dos docentes que ministram aulas e desenvolvam atividades de ensino, pesquisa ou extensão relacionadas ao curso; como na elaboração da proposta orçamentária conforme necessidades do curso;  | subsidiar  a  organização  do  calendário  acadêmico  juntamente  com  a  Direção  de   colaborar na elaboração dos horários de aulas nos semestres letivos, juntamente com   preencher os instrumentos de avaliação, referentes ao curso que coordena, bem como implantar mecanismos de avaliação, atualização e revisão do curso e do PPC;   deferir  as  solicitações  de  matrícula  dos  estudantes  do  curso  feitas  fora  do  prazo, observados  os  critérios  previstos  no  Regulamento  dos  Cursos  de  Graduação  do IFNMG;   acompanhar a ocorrência de evasão, trancamentos  e cancelamentos  de  matrículas  e transferências.   acompanhar | as \n",
            "atividades \n",
            "acadêmicas, \n",
            "o \n",
            "desempenho \n",
            "dos \n",
            "estudantes, \n",
            "os procedimentos \n",
            "referentes \n",
            "à  matrícula,  planejamento  de \n",
            "estudos \n",
            "em \n",
            "situações específicas, assim como o intercâmbio dos estudantes do curso;   acompanhar a flexibilização e adequação curricular para atendimento às pessoas com necessidades específicas; Página 95 de 103\n",
            "\n",
            "  orientar  os  docentes  do  curso  quanto  a  elaboração  e  cumprimento  dos  planos  de ensino das disciplinas; O curso de Ciência da Computação conta com colegiado de curso já instituído cujo objetivo é  desenvolver  atividades  voltadas  para  a  elevação  da  qualidade  do  curso,  com  base  no  Projeto Pedagógico  do  Curso  (PPC),  no  Projeto  Político  Pedagógico  Institucional  (PPI),  no  Plano  de Desenvolvimento  Institucional  (PDI) e na legislação vigente. A  atuação do colegiado do curso de Ciência  da  Computação  se  dará  de  acordo  com  as  normas  definidas  pelo  Regulamento  dos Colegiados dos Cursos de Graduação do IFNMG. De maneira  complementar,  o  Núcleo Docente Estruturante (NDE) do Curso  de Ciência da Computação  atuará  no  acompanhamento,  na  consolidação  e  na  atualização  do  PPC,  realizando estudos e atualização periódica, verificando o impacto do sistema de avaliação de aprendizagem na formação do estudante, e analisando a adequação do perfil do egresso. A atuação do NDE do curso de  Ciência  da  Computação  seguirá  as  normas  definidas  no  Regulamento  do  Núcleo  Docente Estruturante de Curso de Graduação do IFNMG. 11 PERFIL DO CORPO DOCENTE\n",
            "\n",
            "Pergunta: Qual é o tempo máximo permitido para que um estudante conclua o curso\n",
            "\n",
            "\n",
            "\n",
            "FONTES CONSULTADAS (5):\n",
            "   1. [TABLE] ppc_output - Tabela 301\n",
            "      O TCC é composto por um total de 120 horas, oferecidas aos alunos do curso de Ci...\n",
            "   2. [TABLE] ppc_output - Tabela 202\n",
            "      2 IDENTIFICAÇÃO DO CURSO Denominação do Curso: Ciência da Computação Carga horár...\n",
            "   3. [TABLE] estagio_output - Tabela 3\n",
            "      §  3º  Fica  aprovado  o  estágio  em  regime  remoto  ou  teletrabalho  no  âmb...\n",
            "   4. [TABLE] ppc_output - Tabela 310\n",
            "      De acordo com o Regulamento dos Coordenadores de Curso de Graduação do IFNMG, sã...\n",
            "   5. [TABLE] ppc_output - Tabela 312\n",
            "        orientar  os  docentes  do  curso  quanto  a  elaboração  e  cumprimento  dos...\n",
            "\n",
            "============================================================\n",
            "PERGUNTA 3: Como o aluno sabe se foi aprovado ou reprovado numa matéria?\n",
            "============================================================\n",
            "ERRO:index out of range in self\n",
            "\n",
            "============================================================\n",
            "PERGUNTA 4: Em qual momento do curso a gente pode começar a fazer o estágio?\n",
            "============================================================\n",
            "\n",
            "**RESPOSTA:** \n",
            "\n",
            "DA AVALIAÇÃO DO ESTÁGIO\n",
            "\n",
            "DA BOLSA E DO SEGURO DE ESTÁGIO\n",
            "\n",
            "DA ENTIDADE CONCEDENTE DE ESTÁGIO\n",
            "\n",
            "aprovado  pelo  coordenador  de  curso,  ou  cargo  equivalente,  observando  o  percentual  da  carga  horária mínima para estágio no próprio IFNMG, estabelecido no Projeto Pedagógico de cada curso. § 3º O estágio realizado pelo discente nas dependências do próprio IFNMG ou no âmbito da Administração Pública Federal deverá ainda obedecer à Orientação Norma\u0000va nº 02, de 24 de junho esfera federal.\n",
            "\n",
            "DO ESTAGIÁRIO\n",
            "\n",
            "Pergunta: Em qual momento do curso a gente pode começar a fazer o estágio?\n",
            "\n",
            "\n",
            "\n",
            "FONTES CONSULTADAS (5):\n",
            "   1. [ARTICLE] estagio_output - Artigo\n",
            "      DA AVALIAÇÃO DO ESTÁGIO\n",
            "   2. [ARTICLE] estagio_output - Artigo\n",
            "      DA BOLSA E DO SEGURO DE ESTÁGIO\n",
            "   3. [ARTICLE] estagio_output - Artigo\n",
            "      DA ENTIDADE CONCEDENTE DE ESTÁGIO\n",
            "   4. [TABLE] estagio_output - Tabela 4\n",
            "      aprovado  pelo  coordenador  de  curso,  ou  cargo  equivalente,  observando  o ...\n",
            "   5. [ARTICLE] estagio_output - Artigo\n",
            "      DO ESTAGIÁRIO\n",
            "\n",
            "============================================================\n",
            "PERGUNTA 5: Quantas horas e que tipo de atividades contam como Atividades Complementares no curso deCiência da Computação?\n",
            "============================================================\n",
            "\n",
            "**RESPOSTA:** \n",
            "\n",
            "6\n",
            ".5.12 Atividades complementares Montes Claros são atividades obrigatórias a serem desenvolvidas ao longo do curso, no âmbito do Ensino, Pesquisa e Extensão, e estão de acordo com o parágrafo único da Resolução nº 2 de 18 de junho de 2007, segundo o qual “os estágios e atividades complementares dos cursos de graduação, bacharelados,  na  modalidade  presencial,  não  deverão  exceder  a  20%  (vinte  por  cento)  da  carga horária total do curso, salvo no caso de determinações legais em contrário”. Assim,  propõe-se  a  inclusão  de  Atividades  Complementares  no  currículo  do  Curso  de Ciência \n",
            "da  Computação \n",
            "com \n",
            "uma \n",
            "carga \n",
            "horária  mínima \n",
            "de \n",
            "160 \n",
            "horas.  As  Atividades Complementares  visam \n",
            "ampliar  o \n",
            "currículo  pleno  do \n",
            "curso,  possibilitando \n",
            "aos \n",
            "alunos  o aprofundamento \n",
            "temático  e \n",
            "interdisciplinar,  mediante  a \n",
            "realização  de  atividades  e  práticas extracurriculares,  estudos \n",
            "independentes,  participação  em  eventos,  pesquisas  e  atividades  de promoção da cidadania. A \n",
            "realização  das  Atividades  Complementares  deverá  obedecer  ao  Regulamento  das No  Quadro  8,  são  apresentadas  possíveis  Atividades  Complementares,  seus  respectivos documentos comprobatórios e a carga horária máxima para cada atividade.\n",
            "\n",
            "  orientar  os  docentes  do  curso  quanto  a  elaboração  e  cumprimento  dos  planos  de ensino das disciplinas; O curso de Ciência da Computação conta com colegiado de curso já instituído cujo objetivo é  desenvolver  atividades  voltadas  para  a  elevação  da  qualidade  do  curso,  com  base  no  Projeto Pedagógico  do  Curso  (PPC),  no  Projeto  Político  Pedagógico  Institucional  (PPI),  no  Plano  de Desenvolvimento  Institucional  (PDI) e na legislação vigente. A  atuação do colegiado do curso de Ciência  da  Computação  se  dará  de  acordo  com  as  normas  definidas  pelo  Regulamento  dos Colegiados dos Cursos de Graduação do IFNMG. De maneira  complementar,  o  Núcleo Docente Estruturante (NDE) do Curso  de Ciência da Computação  atuará  no  acompanhamento,  na  consolidação  e  na  atualização  do  PPC,  realizando estudos e atualização periódica, verificando o impacto do sistema de avaliação de aprendizagem na formação do estudante, e analisando a adequação do perfil do egresso. A atuação do NDE do curso de  Ciência  da  Computação  seguirá  as  normas  definidas  no  Regulamento  do  Núcleo  Docente Estruturante de Curso de Graduação do IFNMG. 11 PERFIL DO CORPO DOCENTE\n",
            "\n",
            "1) Ciências Básicas: é composto por disciplinas de Matemática, que proporcionam a capa- cidade de abstração, de modelagem e de raciocínio lógico constituindo a base para várias matérias da área de Computação; e de Física, que apresenta e desenvolve a aplicação do método científico. 2) Fundamentos da Computação: abrange o núcleo de matérias que envolvem a teoria e as técnicas fundamentais à formação sólida dos egressos do Bacharelado em Ciência da Computação. 3) Tecnologia da Computação: é composto por um conjunto de matérias que representam uma base de conhecimentos consolidados que capacita o aluno para a elaboração de solução de pro- blemas nos diversos domínios da aplicação. 4) Contexto Social e Profissional: compreende um conjunto de matérias que visa subsidiar a discussão e compreensão da dimensão humana em Ciência da Computação, fornecendo o conhe- cimento sociocultural e organizacional, além de propiciar uma visão humanística das questões soci- ais e profissionais, em consonância com os princípios da ética em computação. 5) Formação Complementar: envolve um conjunto de matérias que representam a consoli- dação das experiências e dos estudos desenvolvidos, além da preparação para a pesquisa científica.\n",
            "\n",
            "6.5 Ementário por disciplina: 6.5.1 1° Período Disciplina: \n",
            "Nº aulas semanais: \n",
            "Carga horária: Introdução à Ciência da Computação \n",
            "4 \n",
            "72 h.a. Ementa: Apresentação  do  curso  de  Ciência  da  Computação:  Áreas  de  formação  e  de  atuação.  Conceitos básicos de computação. Programação Estruturada. Entrada/Saída. Variáveis e Constantes. Operadores e expressões Aritméticos (Simples e compostas). Tipos  de Dados Simples  e Estruturados. Vetores e Matrizes.  Estruturas  condicionais.  Operadores  e  expressões  Relacionais  e  Lógicas.  Estruturas  de Repetição.  Estruturas  de  dados  estáticas.  Funções.  Bibliotecas.  Estudo  de  uma \n",
            "linguagem  de programação. (Sugestão: Linguagem C). Bibliografia Básica: MEDINA, M;  FERTIG,  C. Algoritmos e Programação: Teoria e Prática. Rio de Janeiro: Novatec, 2005. DEITEL, Paul; DEITEL, Harvey. C: Como Programar. 6ª ed. São Paulo: Pearson Brasil, 2011. FOROUZAN,  Behrouz  ;  MOSHARRAF,  Firouz.  Fundamentos  da  Ciência  da  Computação.  São Paulo: Cengage, 2011. Bibliografia Complementar: SCHILDT , H. C Completo e Total. 3. ed. rev. e ampl.. São Paulo: Makron Books,1997. DAMAS, Luis. Linguagem C. 10. ed. São Paulo: LTC, 2007. ZIVIANI, Nivio.  Projeto de Algoritmos  com Implementações em  Pascal  e C.  3ª ed. rev. e ampl. São Paulo: Cengage Learning, 2011. FINGER, Marcelo. Lógica para Computação. São Paulo: Thomson Learning, 2006. 2002. SZWARCFITER, Jayme Luiz. Estruturas de dados e seus algoritmos. 3. ed. rev. São Paulo: LTC, 2010. D\n",
            "isciplina: \n",
            "Nº aulas semanais: \n",
            "Carga horária: Cálculo I \n",
            "6 \n",
            "108 h.a. Ementa: Funções  reais  de  uma  única  variável  real:  funções  afins,  quadráticas,  exponenciais,  logarítmicas, trigonométricas,  modulares.  Limite  e  continuidade.  Derivada:  definição  via \n",
            "limite, \n",
            "regras  de derivação,  diferenciabilidade  e  continuidade,  regra  da  cadeia,  derivada  como \n",
            "taxa  de  variação, diferencial,  derivadas  de  ordem  superior,  regra  de  L’Hôpital,  derivação  implícita,  aplicações  da derivada,  traçado  de  gráficos,  máximos  e  mínimos,  Teorema  de  Rolle,  Teorema  do  Valor  Médio. Integral: Somas de Riemann, Teorema Fundamental do Cálculo, Técnicas de Integração, Aplicações de Integral, Integração imprópria. Bibliografia Básica: STEWART, J. Cálculo. vol. 1. 8 ed. São Paulo: Cengage Learning, 2017. ANTON, Howard. et al.; Cálculo. Vol.1. 10 ed. São Paulo:Bookman, 2014. THOMAS, G. B. et al. Cálculo. vol.1. 10 ed. São Paulo: Person, 2014. Bibliografia Complementar: LEITHOLD, Louis.; Cálculo com Geometria Analítica. vol. 1 ed. São Paulo: Harbra, 1994.\n",
            "\n",
            "  preservação do meio ambiente. Montes Claros todos os princípios descritos acima se integram por meio do Ensino, da Pesquisa e da  Extensão,  dimensões  indissociáveis  e  que  estão  presentes  em  todas  as  atividades  do  Curso, mesmo  que  em  algumas  delas  seja  realçada  uma  dessas  dimensões.    Desse  modo,  as Atividades Complementares, por exemplo, proporcionam ao discente a oportunidade de participar de práticas de  extensão,  articulando  conhecimentos  apropriados  ou  produzidos  no  Curso  às  demandas  das comunidades  parceiras.  Já  a  pesquisa  encontra  espaço  privilegiado  nos  programas  de  Iniciação Científica e na construção do Trabalho de Conclusão de Curso (TCC). Em  um  mundo  em  que  a  velocidade  das  transformações  sociais  e  tecnológicas  é  cada  vez maior,  e  mais  rapidamente  se  tornam  obsoletas  algumas  práticas  do  passado,  optou-se  por  um projeto pedagógico baseado no Aprender a aprender, um dos quatro pilares da educação, segundo o Relatório para a UNESCO da Comissão Internacional  sobre Educação no Século  XXI, Educação: Um Tesouro a Descobrir (DELORS et al, 1996). Outros dois itens basilares para educação, citados pelo referido relatório, Aprender a Conviver e Aprender a Ser também são destacados dos princípios norteadores nos quais se fundamenta o Curso, que buscam contemplar a universalização do acesso ao  conhecimento  e  à  informação  e  aspectos  associados  a  uma  formação  pautada  na  ética  e  no respeito à vida. Neste contexto, o Bacharelado em Ciência da Computação propõe formar recursos humanos com sólidos fundamentos na área, capacidade de autoaprendizagem e prosseguimento dos estudos, com vistas a atender às necessidades da sociedade, por meio da produção e utilização responsável de tecnologias que melhorem a qualidade de vida da população.\n",
            "\n",
            "Pergunta: Quantas horas e que tipo de atividades contam como Atividades Complementares no curso deCiência da Computação?\n",
            "\n",
            "\n",
            "\n",
            "FONTES CONSULTADAS (5):\n",
            "   1. [TABLE] ppc_output - Tabela 297\n",
            "      6\n",
            ".5.12 Atividades complementares Montes Claros são atividades obrigatórias a se...\n",
            "   2. [TABLE] ppc_output - Tabela 312\n",
            "        orientar  os  docentes  do  curso  quanto  a  elaboração  e  cumprimento  dos...\n",
            "   3. [TABLE] ppc_output - Tabela 211\n",
            "      1) Ciências Básicas: é composto por disciplinas de Matemática, que proporcionam ...\n",
            "   4. [TABLE] ppc_output - Tabela 230\n",
            "      6.5 Ementário por disciplina: 6.5.1 1° Período Disciplina: \n",
            "Nº aulas semanais: \n",
            "...\n",
            "   5. [TABLE] ppc_output - Tabela 206\n",
            "        preservação do meio ambiente. Montes Claros todos os princípios descritos aci...\n",
            "\n",
            "SALVANDO RESULTADOS\n",
            "Resultados salvos em 'rag_results_safe.json'\n"
          ]
        }
      ],
      "source": [
        "def ask_rag_question(qa_chain, question: str, question_num: int = None):\n",
        "    \"\"\"Faz uma pergunta ao pipeline RAG\"\"\"\n",
        "\n",
        "    if question_num:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"PERGUNTA {question_num}: {question}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        import signal\n",
        "        import time\n",
        "\n",
        "        def timeout_handler(signum, frame):\n",
        "            raise TimeoutError(\"Tempo limite excedido\")\n",
        "\n",
        "        signal.signal(signal.SIGALRM, timeout_handler)\n",
        "        signal.alarm(190)\n",
        "\n",
        "        try:\n",
        "            result = qa_chain({\"query\": question})\n",
        "            signal.alarm(0)\n",
        "        except TimeoutError:\n",
        "            signal.alarm(0)\n",
        "            raise TimeoutError(\"Processamento demorou muito tempo\")\n",
        "\n",
        "\n",
        "        cleaned_result = result['result'].strip()  # remove espaços e \\n do início/fim\n",
        "        if cleaned_result.startswith('\\n'):\n",
        "            cleaned_result = cleaned_result[1:]  # remove primeiro \\n se existir\n",
        "\n",
        "\n",
        "        print(f\"\\n**RESPOSTA:** {result['result']}\")\n",
        "\n",
        "\n",
        "        if result.get('source_documents'):\n",
        "            print(f\"\\nFONTES CONSULTADAS ({len(result['source_documents'])}):\")\n",
        "            for i, doc in enumerate(result['source_documents'], 1):\n",
        "                source_type = doc.metadata.get('content_type', 'documento').upper()\n",
        "                print(f\"   {i}. [{source_type}] {doc.metadata['source']}\")\n",
        "\n",
        "\n",
        "                preview = doc.page_content[:80] + \"...\" if len(doc.page_content) > 80 else doc.page_content\n",
        "                print(f\"      {preview}\")\n",
        "        else:\n",
        "            print(f\"\\nAVISO: Nenhuma fonte foi recuperada para esta pergunta\")\n",
        "\n",
        "        return {\n",
        "            \"result\": cleaned_result,  # Retorne a versão limpa\n",
        "            \"source_documents\": result.get('source_documents', [])\n",
        "        }\n",
        "\n",
        "    except TimeoutError as e:\n",
        "        print(f\"ERRO DE TIMEOUT: {e}\")\n",
        "        return {\n",
        "            \"result\": \"Desculpe, a pergunta demorou muito para processar. Tente reformular ou perguntar algo mais específico.\",\n",
        "            \"source_documents\": []\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO:{e}\")\n",
        "        return {\n",
        "            \"result\": f\"Erro ao processar a pergunta: {str(e)}\",\n",
        "            \"source_documents\": []\n",
        "        }\n",
        "\n",
        "\n",
        "def run_rag():\n",
        "    \"\"\"Executa pipeline RAG\"\"\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        # llm\n",
        "\n",
        "        llm = setup_dialogpt_model()\n",
        "\n",
        "        # rag\n",
        "\n",
        "        qa_chain = setup_rag_pipeline(vector_manager, llm)\n",
        "\n",
        "        # perguntas\n",
        "        test_questions = [\n",
        "            \"Quais são os professores que podem ser orientadores do TCC?\",\n",
        "            \"Qual é o tempo máximo permitido para que um estudante conclua o curso\",\n",
        "            \"Como o aluno sabe se foi aprovado ou reprovado numa matéria?\",\n",
        "            \"Em qual momento do curso a gente pode começar a fazer o estágio?\",\n",
        "            \"Quantas horas e que tipo de atividades contam como Atividades Complementares no curso deCiência da Computação?\"\n",
        "        ]\n",
        "\n",
        "        results = []\n",
        "        for i, question in enumerate(test_questions, 1):\n",
        "            result = ask_rag_question(qa_chain, question, i)\n",
        "            results.append({\n",
        "                \"question\": question,\n",
        "                \"answer\": result[\"result\"],\n",
        "                \"sources\": [doc.metadata for doc in result.get(\"source_documents\", [])]\n",
        "            })\n",
        "            # espaço\n",
        "            import time\n",
        "            time.sleep(2)\n",
        "\n",
        "\n",
        "        print(f\"\\nSALVANDO RESULTADOS\")\n",
        "        with open('rag_results.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(\"Resultados salvos em 'rag_results.json'\")\n",
        "\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO {e}\")\n",
        "        return []\n",
        "\n",
        "# EXECUTAR TUDO\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_rag()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Na primeira consulta sobre professores orientadores, o modelo devolveu informações sobre responsabilidades dos alunos em TCC; na segunda pergunta sobre tempo máximo de conclusão do curso, retornou detalhes sobre carga horária do TCC; e na quarta questão sobre estágio, apresentou trechos burocráticos de avaliação em vez do momento de início; na quinta retornou a quantidade necessária para horas de atividade complementar e quais atividades contam como atividade complementar. Essas inconsistências revelam que, embora o mecanismo de recuperção esteja encontrando textos com similaridade lexical, falta precisão na seleção contextual. Todos os trechos vieram do documento, de forma bem similar ou igual. Torna-se importante, portanto, chegar melhor os parametros do modelo LLM. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LinkCOLAB: https://colab.research.google.com/drive/1x7KN_gsf_fNPKcQczMA6pILGBJag5sSm?usp=sharing"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
    
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
